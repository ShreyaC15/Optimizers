{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP_Custom_Optimizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShreyaC15/Optimizers/blob/main/MLP_Custom_Optimizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71kdFp0QgF4K"
      },
      "source": [
        "# IST597:- Multi-Layer Perceptron - Custom Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2yHcl5xgPV1"
      },
      "source": [
        "## Load the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DPwxLR2gSLC"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "seeds=2785                     # Update seeds\n",
        "np.random.seed(seeds)\n",
        "tf.random.set_seed(seeds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wV-3kEaggcO8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79639f2f-1190-410e-ef56-7151a913ace0"
      },
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Dk_S2TMg_6_"
      },
      "source": [
        "## Load data and perform pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "iYKrtyf7uqzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = tf.cast(tf.reshape(X_train, (-1, X_train.shape[1]*X_train.shape[2])), dtype=tf.float32)\n",
        "X_test = tf.cast(tf.reshape(X_test, (-1, X_test.shape[1]*X_test.shape[2])), dtype=tf.float32)"
      ],
      "metadata": {
        "id": "L4M5F3SB6Xrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train/255.0\n",
        "X_test = X_test/255.0"
      ],
      "metadata": {
        "id": "yJmGJ_xiytwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_val = X_train[-10000:]\n",
        "y_val = y_train[-10000:]"
      ],
      "metadata": {
        "id": "NNxiC2uuHAHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = tf.keras.utils.to_categorical(y_train)\n",
        "y_test = tf.keras.utils.to_categorical(y_test)\n",
        "y_val=tf.keras.utils.to_categorical(y_val)"
      ],
      "metadata": {
        "id": "JP9rfC59IkKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aigqKFFF5BM2"
      },
      "source": [
        "# Split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(100)\n",
        "validate_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(100)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_size = y_train.shape[1]"
      ],
      "metadata": {
        "id": "qePB4CYDJjI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb4hOoVbnzSJ"
      },
      "source": [
        "## Build MLP using Eager Execution"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden1,size_hidden2, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1,size_hidden2, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden1]))\n",
        "\n",
        "    # Initialize weights between hidden 1 and hidden layer 2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer 2\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden2]))\n",
        "\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]    \n",
        "\n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    \"\"\"\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    \"\"\"\n",
        "\n",
        "    #y_pred is the softmax value\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf)\n",
        "\n",
        "  def backward(self, X_train, y_train,m,v,u,t):\n",
        "    \"\"\"\n",
        "    Backward pass\n",
        "    \"\"\"    \n",
        "    alpha=1e-4\n",
        "    beta1=0.9\n",
        "    beta2=0.999\n",
        "    beta3=0.999987\n",
        "    eps=1e-8\n",
        "    t=t+1\n",
        "  \n",
        "    with tf.GradientTape() as tape:\n",
        "      tape.watch(self.variables)\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    g=grads\n",
        "    \n",
        "    m = [(beta1 * i)+((1-beta1)* j) for i,j in zip(m,g)]\n",
        "    v=  [(beta2 * i)+((1-beta2)* j**2) for i,j in zip(v,g)]\n",
        "    u=  [(beta3 * i)+((1-beta3)* j**3) for i,j in zip(u,g)]\n",
        "    mb = [i / (1 - beta1**(t)) for i in  m]\n",
        "    vb = [i / (1 - beta2**(t)) for i in  v]\n",
        "    ub = [i / (1 - beta3**(t)) for i in  u]\n",
        "    temp= [i**(1/2) + eps*j**(1/3) for i,j in zip(vb,ub)]\n",
        "    temp2=[alpha*i/j for i,j in zip(m,temp)]\n",
        "    Wt = [a_i - b_i for a_i, b_i in zip(self.variables, temp2)]\n",
        "    \n",
        "    W1=self.variables[0]\n",
        "    W1.assign(Wt[0])\n",
        "    W2=self.variables[1]\n",
        "    W2.assign(Wt[1])\n",
        "    W3=self.variables[2]\n",
        "    W3.assign(Wt[2])\n",
        "    b1=self.variables[3]\n",
        "    b1.assign(Wt[3])\n",
        "    b2=self.variables[4]\n",
        "    b2.assign(Wt[4])\n",
        "    b3=self.variables[5]\n",
        "    b3.assign(Wt[5])\n",
        "\n",
        "    return g,m,v,u,t\n",
        "\n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    # Compute values in hidden layer\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    return tf.nn.softmax(output)\n",
        "\n",
        "  def var(self,y_pred):\n",
        "    \"\"\"\n",
        "    Calculate variance \n",
        "    \"\"\"\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    std_dev = np.std(y_pred_tf) #Calculates standard deviation\n",
        "    variance = (std_dev**2) # calculate variance\n",
        "    return variance\n"
      ],
      "metadata": {
        "id": "wr7pJ3JmPcn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# L2 regularization\n",
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden1,size_hidden2, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1,size_hidden2, size_output, device\n",
        "    \n",
        "    # Initialize weights between input layer and hidden layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden1]))\n",
        "\n",
        "    # Initialize weights between hidden 1 and hidden layer 2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer 2\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden2]))\n",
        "\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]    \n",
        "\n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "    return self.y\n",
        "  \n",
        "  def loss(self, y_pred, y_true):\n",
        "    \"\"\"\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    \"\"\"\n",
        "\n",
        "    #y_pred is the softmax value\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.keras.losses.CategoricalCrossentropy()(y_true_tf, y_pred_tf) + tf.reduce_mean((self.W1)**2) + tf.reduce_mean((self.W2)**2) + tf.reduce_mean((self.W3)**2)\n",
        "\n",
        "  def backward(self, X_train, y_train,m,v,u,t):\n",
        "    \"\"\"\n",
        "    Backward Pass\n",
        "    \"\"\"    \n",
        "    alpha=1e-4\n",
        "    beta1=0.9\n",
        "    beta2=0.999\n",
        "    beta3=0.999987\n",
        "    eps=1e-8\n",
        "    t=t+1\n",
        "  \n",
        "    with tf.GradientTape() as tape:\n",
        "      tape.watch(self.variables)\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    g=grads\n",
        "    \n",
        "    m = [(beta1 * i)+((1-beta1)* j) for i,j in zip(m,g)]\n",
        "    v=  [(beta2 * i)+((1-beta2)* j**2) for i,j in zip(v,g)]\n",
        "    u=  [(beta3 * i)+((1-beta3)* j**3) for i,j in zip(u,g)]\n",
        "    mb = [i / (1 - beta1**(t)) for i in  m]\n",
        "    vb = [i / (1 - beta2**(t)) for i in  v]\n",
        "    ub = [i / (1 - beta3**(t)) for i in  u]\n",
        "    temp= [i**(1/2) + eps*j**(1/3) for i,j in zip(vb,ub)]\n",
        "    temp2=[alpha*i/j for i,j in zip(m,temp)]\n",
        "    Wt = [a_i - b_i for a_i, b_i in zip(self.variables, temp2)]\n",
        "    \n",
        "    W1=self.variables[0]\n",
        "    W1.assign(Wt[0])\n",
        "    W2=self.variables[1]\n",
        "    W2.assign(Wt[1])\n",
        "    W3=self.variables[2]\n",
        "    W3.assign(Wt[2])\n",
        "    b1=self.variables[3]\n",
        "    b1.assign(Wt[3])\n",
        "    b2=self.variables[4]\n",
        "    b2.assign(Wt[4])\n",
        "    b3=self.variables[5]\n",
        "    b3.assign(Wt[5])\n",
        "\n",
        "    return g,m,v,u,t\n",
        "\n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    # Compute values in hidden layer\n",
        "    what1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    return tf.nn.softmax(output)\n",
        "\n",
        "  def var(self,y_pred):\n",
        "    \"\"\"\n",
        "    Calculate variance \n",
        "    \"\"\"\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    std_dev = np.std(y_pred_tf) #Calculates standard deviation\n",
        "    variance = (std_dev**2) # calculate variance\n",
        "    return variance\n"
      ],
      "metadata": {
        "id": "lpEarspOPnVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUDFOuNk618X"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZPVUu0YDa-_"
      },
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdMFAuH18Ve0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c97be0a7-874a-48a0-cdaf-5e4c2b870f88"
      },
      "source": [
        "# Initialize model using GPU\n",
        "mlp_on_gpu = MLP(X_train.shape[1], 256,128, label_size, device='gpu')\n",
        "\n",
        "train_loss = []\n",
        "val_loss=[]\n",
        "seed_ = []\n",
        "train_accuracy_ = []\n",
        "train_var_ =[]\n",
        "val_accuracy_ = []\n",
        "val_var_ =[]\n",
        "time_start = time.time()\n",
        "m=[1,1,1,1,1,1]\n",
        "v=[1,1,1,1,1,1]\n",
        "u=[1,1,1,1,1,1]\n",
        "t=0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  val_loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
        "\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(100, seed=epoch*(seeds)).batch(100)\n",
        "  validate_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val)).shuffle(100, seed=epoch*(seeds)).batch(100)\n",
        "\n",
        "  #creating y_train and y_val after each shuffled data above\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs)\n",
        "    loss_total_gpu = loss_total_gpu + mlp_on_gpu.loss(preds, outputs)\n",
        "    g,m,v,u,t=mlp_on_gpu.backward(inputs, outputs,m,v,u,t)\n",
        "\n",
        "  y_batch_train = y_train\n",
        "  result=mlp_on_gpu.forward(X_train)\n",
        "  correct_prediction = tf.equal(tf.round(y_batch_train,1), tf.round(result,1))\n",
        "  train_accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  train_accuracy_.append(train_accuracy*100)\n",
        "  #train_var = mlp_on_gpu.var(correct_prediction)\n",
        "  #train_var_.append(train_var)\n",
        "\n",
        "  for inputs, outputs in validate_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs)\n",
        "    val_loss_total = val_loss_total + mlp_on_gpu.loss(preds, outputs)\n",
        "    \n",
        "  y_batch_val = y_val\n",
        "  val_result = mlp_on_gpu.forward(x_val)\n",
        "  correct_val_prediction = tf.equal(tf.round(y_batch_val,1), tf.round(val_result,1))\n",
        "  validation_accuracy = tf.reduce_mean(tf.cast(correct_val_prediction, \"float\"))\n",
        "  val_accuracy_.append(validation_accuracy*100)\n",
        "  #val_var = mlp_on_gpu.var(correct_val_prediction)\n",
        "  #val_var_.append(val_var)\n",
        "\n",
        "  print('Number of Epoch = {} - Average train CCE:= {}'.format(epoch + 1, np.sum(loss_total_gpu) / X_train.shape[0]))\n",
        "  print('Number of Epoch = {} - Average val CCE:= {}'.format(epoch + 1, np.sum(val_loss_total) / x_val.shape[0]))\n",
        "  print()\n",
        "  print('Train Accuracy = {}'.format(train_accuracy*100))\n",
        "  print('Val Accuracy = {}'.format(validation_accuracy*100))\n",
        "  print()\n",
        "  #print('Train Variance = {}'.format(train_var))\n",
        "  #print('Val Variance = {}'.format(val_var))\n",
        "  print(\"================================================================\")\n",
        "\n",
        "  train_loss.append(float(loss_total_gpu))\n",
        "  val_loss.append(float(val_loss_total))\n",
        "  seed_.append((epoch+1)*(2785))\n",
        "  \n",
        "time_taken = time.time() - time_start\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Average train CCE:= 0.028805645751953126\n",
            "Number of Epoch = 1 - Average val CCE:= 0.0237332763671875\n",
            "\n",
            "Train Accuracy = 90.21083068847656\n",
            "Val Accuracy = 90.2030029296875\n",
            "\n",
            "================================================================\n",
            "Number of Epoch = 2 - Average train CCE:= 0.02192411092122396\n",
            "Number of Epoch = 2 - Average val CCE:= 0.020277687072753906\n",
            "\n",
            "Train Accuracy = 90.09266662597656\n",
            "Val Accuracy = 90.08000183105469\n",
            "\n",
            "================================================================\n",
            "Number of Epoch = 3 - Average train CCE:= 0.018938871256510417\n",
            "Number of Epoch = 3 - Average val CCE:= 0.01744588165283203\n",
            "\n",
            "Train Accuracy = 90.27983856201172\n",
            "Val Accuracy = 90.26300048828125\n",
            "\n",
            "================================================================\n",
            "Number of Epoch = 4 - Average train CCE:= 0.016152950032552082\n",
            "Number of Epoch = 4 - Average val CCE:= 0.014739244079589844\n",
            "\n",
            "Train Accuracy = 90.6901626586914\n",
            "Val Accuracy = 90.67900085449219\n",
            "\n",
            "================================================================\n",
            "Number of Epoch = 5 - Average train CCE:= 0.013582737223307292\n",
            "Number of Epoch = 5 - Average val CCE:= 0.012345975494384766\n",
            "\n",
            "Train Accuracy = 91.9451675415039\n",
            "Val Accuracy = 91.96599578857422\n",
            "\n",
            "================================================================\n",
            "Number of Epoch = 6 - Average train CCE:= 0.011433817545572917\n",
            "Number of Epoch = 6 - Average val CCE:= 0.010482322692871094\n",
            "\n",
            "Train Accuracy = 93.15966796875\n",
            "Val Accuracy = 93.21399688720703\n",
            "\n",
            "================================================================\n",
            "Number of Epoch = 7 - Average train CCE:= 0.009827017211914063\n",
            "Number of Epoch = 7 - Average val CCE:= 0.009127634429931641\n",
            "\n",
            "Train Accuracy = 94.08733367919922\n",
            "Val Accuracy = 94.0999984741211\n",
            "\n",
            "================================================================\n",
            "Number of Epoch = 8 - Average train CCE:= 0.008651348876953124\n",
            "Number of Epoch = 8 - Average val CCE:= 0.008132569885253906\n",
            "\n",
            "Train Accuracy = 94.7768325805664\n",
            "Val Accuracy = 94.78199768066406\n",
            "\n",
            "================================================================\n",
            "Number of Epoch = 9 - Average train CCE:= 0.007772755940755208\n",
            "Number of Epoch = 9 - Average val CCE:= 0.007380976867675781\n",
            "\n",
            "Train Accuracy = 95.21316528320312\n",
            "Val Accuracy = 95.20700073242188\n",
            "\n",
            "================================================================\n",
            "Number of Epoch = 10 - Average train CCE:= 0.007095135498046875\n",
            "Number of Epoch = 10 - Average val CCE:= 0.0067947265625\n",
            "\n",
            "Train Accuracy = 95.52149963378906\n",
            "Val Accuracy = 95.47100067138672\n",
            "\n",
            "================================================================\n",
            "\n",
            "Total time taken (in seconds): 186.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXe-2MENCOjq"
      },
      "source": [
        "## One Step Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKxWn7CNDVN5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f16bd82b-216b-419b-8dda-229514d08875"
      },
      "source": [
        "test_loss_total =tf.zeros([1,1], dtype=tf.float32)\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_gpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_gpu.loss(preds, outputs)\n",
        "test_result = mlp_on_gpu.forward(X_test)\n",
        "test_batch = y_test\n",
        "  \n",
        "correct_test_prediction = tf.equal(tf.round(test_batch), tf.round(test_result))\n",
        "test_accuracy = tf.reduce_mean(tf.cast(correct_test_prediction, \"float\"))\n",
        "#test_variance = mlp_on_gpu.var(correct_test_prediction)\n",
        "print('Test Accuracy = {}'.format(test_accuracy*100))\n",
        "print('Test Average CCE: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "#print('Test Variance = {}'.format(test_variance*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy = 95.39700317382812\n",
            "Test Average CCE: 0.0434\n"
          ]
        }
      ]
    }
  ]
}